A continuación te presento un ejemplo ampliado que incorpora manejo de errores y la integración con un sistema de monitoreo basado en Prometheus. Este ejemplo actualiza el módulo de actualización dinámica para OLA, de modo que cualquier error se registre mediante el sistema de logging y se incremente un contador de errores en Prometheus. Además, se actualizan métricas para los paquetes nuevos, actualizados y eliminados.
# File: /src/providers/ola_dynamic_updater.py

import asyncio
import hashlib
import logging
from datetime import datetime
from typing import List, Dict
from pydantic import BaseModel, Field
from prometheus_client import Gauge, Counter
from bs4 import BeautifulSoup

# Configuración básica del logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Intentamos importar LRUCache desde cachetools; si no, definimos una versión simple.
try:
    from cachetools import LRUCache
except ImportError:
    class LRUCache(dict):
        def __init__(self, maxsize):
            self.maxsize = maxsize
        def get(self, key, default=None):
            return super().get(key, default)

# ---------------------------
# Sistema de Monitoreo con Prometheus
# ---------------------------
class OLAMonitor:
    def __init__(self):
        self.paquetes_nuevos = Gauge('ola_new_packages', 'Cantidad de paquetes nuevos detectados')
        self.paquetes_actualizados = Gauge('ola_updated_packages', 'Cantidad de paquetes actualizados detectados')
        self.paquetes_eliminados = Gauge('ola_deleted_packages', 'Cantidad de paquetes eliminados detectados')
        self.error_counter = Counter('ola_update_errors', 'Cantidad de errores en la actualización de OLA')

    def update_metrics(self, report: Dict):
        stats = report.get('stats', {})
        self.paquetes_nuevos.set(stats.get('total_nuevos', 0))
        self.paquetes_actualizados.set(stats.get('total_actualizados', 0))
        self.paquetes_eliminados.set(stats.get('total_eliminados', 0))

    def log_error(self):
        self.error_counter.inc()

# ---------------------------
# Modelos de Datos con Pydantic
# ---------------------------
class Vuelo(BaseModel):
    salida: str
    llegada: str = None
    escala: str = None
    espera: str = None
    duracion: str

class ImpuestoEspecial(BaseModel):
    nombre: str
    monto: str
    detalle: str

class PoliticasCancelacion(BaseModel):
    periodo_61_noches: str = Field(..., alias="0-61 noches antes")
    periodo_62_90: str = Field(..., alias="62-90 noches antes")
    periodo_91_plus: str = Field(..., alias="91+ noches antes")

class PaqueteOLA(BaseModel):
    destino: str
    aerolinea: str
    origen: str
    duracion: int
    moneda: str
    incluye: List[str]
    fechas: List[datetime]
    precio: float
    impuestos: float
    politicas_cancelacion: PoliticasCancelacion
    vuelos: List[Vuelo]
    cotizacion_especial: Dict[str, List[ImpuestoEspecial]] = None
    restricciones: str = None
    assist_card: Dict = None
    data_hash: str = None  # Campo adicional para identificar cambios

# ---------------------------
# Simulación de SmartBrowser (Reemplaza por la implementación real)
# ---------------------------
class SmartBrowser:
    def __init__(self, headless=True, proxy=None, user_agent=None):
        self.headless = headless
        self.proxy = proxy
        self.user_agent = user_agent

    async def navigate(self, url: str):
        logger.info(f"Navegando a {url}")
        await asyncio.sleep(1)

    async def wait_for_element(self, selector: str, timeout: int = 10):
        logger.info(f"Esperando el elemento {selector} durante {timeout} segundos")
        await asyncio.sleep(1)

    async def execute_script(self, script: str):
        logger.info("Ejecutando script de extracción de datos")
        await asyncio.sleep(1)
        # Retornamos datos simulados para el ejemplo
        return [
            {
                "destino": "Cancún",
                "aerolinea": "AeroMéxico",
                "origen": "Ciudad de México",
                "duracion": 5,
                "moneda": "MXN",
                "incluye": ["Vuelo", "Hotel"],
                "fechas": ["25-12-2025", "26-12-2025"],
                "precio": "3500.50",
                "impuestos": 200.0,
                "politicas_cancelacion": {
                    "0-61 noches antes": "Reembolso completo",
                    "62-90 noches antes": "50% reembolso",
                    "91+ noches antes": "No reembolsable"
                },
                "vuelos": [
                    {
                        "salida": "08:00",
                        "llegada": "10:00",
                        "escala": None,
                        "espera": None,
                        "duracion": "2h"
                    }
                ]
            }
        ]

    async def close(self):
        logger.info("Cerrando SmartBrowser")
        await asyncio.sleep(0.5)

# ---------------------------
# Clase OLAUpdater: Actualización Dinámica de Datos de OLA
# ---------------------------
class OLAUpdater:
    def __init__(self, config: Dict, monitor: OLAMonitor = None):
        self.config = config
        self.browser = SmartBrowser(
            headless=config.get('headless', True),
            proxy=config.get('proxy'),
            user_agent=config.get('user_agent')
        )
        self.base_url = config.get('base_url', 'https://ola.com')
        self.cache = LRUCache(maxsize=1000)
        self.data_version = 1.0
        self.monitor = monitor

    async def fetch_data(self, destino: str) -> Dict:
        try:
            raw_data = await self._scrape_data(destino)
            normalized = self._normalize_data(raw_data)
            changes = self._detect_changes(normalized)
            report = self._process_updates(changes)
            if self.monitor:
                self.monitor.update_metrics(report)
            return report
        except Exception as e:
            self._log_error(f"Error en fetch_data: {str(e)}")
            raise

    async def _scrape_data(self, destino: str) -> List[Dict]:
        try:
            url = f"{self.base_url}/busqueda?destino={destino}"
            await self.browser.navigate(url)
            await self.browser.wait_for_element('.paquete-container', timeout=10)
            script = """
            () => {
                const paquetes = [];
                document.querySelectorAll('.paquete-item').forEach(item => {
                    const paquete = {}; // Aquí iría la lógica real de extracción
                    paquetes.push(paquete);
                });
                return paquetes;
            }
            """
            data = await self.browser.execute_script(script)
            await self.browser.close()
            return data
        except Exception as e:
            self._log_error(f"Error en _scrape_data: {str(e)}")
            raise

    def _normalize_data(self, raw_data: List[Dict]) -> List[PaqueteOLA]:
        validated = []
        for item in raw_data:
            try:
                # Convertir las fechas al objeto datetime
                item['fechas'] = [datetime.strptime(f, "%d-%m-%Y") for f in item['fechas']]
                item['precio'] = float(item['precio'])
                # Generar un hash único para identificar cambios
                item['data_hash'] = self._generate_hash(item)
                paquete = PaqueteOLA(**item)
                validated.append(paquete)
            except Exception as e:
                self._log_error(f"Error normalizando paquete: {str(e)}")
        return validated

    def _generate_hash(self, data: Dict) -> str:
        # Convertir las fechas a string para el hash
        fechas_str = "-".join(data['fechas'])
        hash_input = f"{data['destino']}{data['precio']}{fechas_str}"
        return hashlib.sha256(hash_input.encode()).hexdigest()

    def _detect_changes(self, new_data: List[PaqueteOLA]) -> Dict:
        changes = {'nuevos': [], 'actualizados': [], 'eliminados': []}
        try:
            for paquete in new_data:
                cached = self.cache.get(paquete.data_hash)
                if not cached:
                    changes['nuevos'].append(paquete)
                    self.cache[paquete.data_hash] = paquete
                elif paquete != cached:
                    changes['actualizados'].append(paquete)
                    self.cache[paquete.data_hash] = paquete
            # Detectar eliminados comparando con las claves en cache
            current_hashes = {p.data_hash for p in new_data}
            eliminados = [h for h in list(self.cache.keys()) if h not in current_hashes]
            changes['eliminados'] = eliminados
        except Exception as e:
            self._log_error(f"Error en _detect_changes: {str(e)}")
        return changes

    def _process_updates(self, changes: Dict) -> Dict:
        report = {
            'stats': {
                'total_nuevos': len(changes['nuevos']),
                'total_actualizados': len(changes['actualizados']),
                'total_eliminados': len(changes['eliminados'])
            },
            'details': changes
        }
        return report

    def _log_error(self, message: str):
        logger.error(message)
        if self.monitor:
            self.monitor.log_error()

# ---------------------------
# Integración con el Agente Principal
# ---------------------------
class TravelAgent:
    def __init__(self):
        # Configuración central del componente OLA
        self.ola_config = {
            'base_url': 'https://ola.com',
            'headless': True,
            'update_interval': 3600  # intervalo en segundos (1 hora)
        }
        # Instanciar el monitor de OLA
        self.monitor = OLAMonitor()
        self.ola_updater = OLAUpdater(self.ola_config, monitor=self.monitor)
        # La base de conocimientos se simula con un diccionario
        self.knowledge_base = {}

    async def update_ola_data(self, destino: str):
        try:
            results = await self.ola_updater.fetch_data(destino)
            self.apply_updates(results)
            if results['stats']['total_actualizados'] > 0:
                self._trigger_analysis(reanalyze=True)
        except Exception as e:
            logger.error(f"Fallo en update_ola_data: {str(e)}")

    def apply_updates(self, results: Dict):
        logger.info("Aplicando actualizaciones en la base de conocimientos...")
        # Aquí se integraría la lógica para actualizar la base de conocimientos
        self.knowledge_base.update(results)
        logger.info(f"Actualización: {results['stats']}")

    def _trigger_analysis(self, reanalyze: bool):
        logger.info("Disparando análisis adicional debido a actualizaciones significativas...")

# ---------------------------
# Ejemplo de Uso
# ---------------------------
async def main():
    agent = TravelAgent()
    # Actualizar datos para el destino "Cancún"
    await agent.update_ola_data("Cancún")
    # Para una ejecución periódica se puede usar:
    # while True:
    #     await agent.update_ola_data("Cancún")
    #     await asyncio.sleep(agent.ola_config['update_interval'])

if __name__ == "__main__":
    asyncio.run(main())

Explicación del Código Ampliado:
Manejo de Errores:


Cada método crítico en la actualización (por ejemplo, _scrape_data, _normalize_data y fetch_data) está envuelto en bloques try/except que capturan excepciones.
El método _log_error registra el error mediante logger.error() y, si está disponible, incrementa un contador de errores en el monitor (usando Prometheus).
Integración con el Sistema de Monitoreo:


La clase OLAMonitor define métricas de Prometheus: tres Gauge para el número de paquetes nuevos, actualizados y eliminados, y un Counter para errores.
En el método fetch_data de OLAUpdater, una vez procesados los datos, se actualizan las métricas llamando a monitor.update_metrics(report).
El agente principal (TravelAgent) instancia el monitor y lo pasa al updater, permitiendo que tanto la actualización como el manejo de errores se integren en el monitoreo.
Este ejemplo se puede adaptar y ampliar según los requerimientos específicos de tu sistema, integrando de forma centralizada la gestión de errores y las métricas de monitoreo. ¿Te gustaría explorar algún otro aspecto, como la configuración del servidor Prometheus o cómo exponer estas métricas a través de una API REST?
